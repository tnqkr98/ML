{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading the train img Complete : 1027\n",
      "loading the validation img Complete : 256\n",
      "초기 input shape\n",
      "(10000, 1027)\n",
      "초기 weight1 shape\n",
      "(10, 10000)\n",
      "초기 weight2 shape\n",
      "(10, 10)\n",
      "초기 weight3 shape\n",
      "(10,)\n",
      "epoch: 0 , Train_loss : 1.048729, TrainAcc : 0.507303 , Val loss : 0.742474 , Val acc : 0.414062, time : 0.16256s\n",
      "epoch: 10 , Train_loss : 0.909366, TrainAcc : 0.471276 , Val loss : 0.975279 , Val acc : 0.500000, time : 1.62418s\n",
      "epoch: 20 , Train_loss : 0.862992, TrainAcc : 0.521908 , Val loss : 0.921988 , Val acc : 0.500000, time : 1.56982s\n",
      "epoch: 30 , Train_loss : 0.827974, TrainAcc : 0.546251 , Val loss : 0.874758 , Val acc : 0.500000, time : 1.65791s\n",
      "epoch: 40 , Train_loss : 0.800515, TrainAcc : 0.583252 , Val loss : 0.832781 , Val acc : 0.500000, time : 1.67255s\n",
      "epoch: 50 , Train_loss : 0.778204, TrainAcc : 0.617332 , Val loss : 0.795423 , Val acc : 0.500000, time : 1.57283s\n",
      "epoch: 60 , Train_loss : 0.759529, TrainAcc : 0.641675 , Val loss : 0.762148 , Val acc : 0.500000, time : 1.57330s\n",
      "epoch: 70 , Train_loss : 0.743528, TrainAcc : 0.655307 , Val loss : 0.732470 , Val acc : 0.542969, time : 1.56886s\n",
      "epoch: 80 , Train_loss : 0.729560, TrainAcc : 0.669912 , Val loss : 0.705953 , Val acc : 0.574219, time : 1.57580s\n",
      "epoch: 90 , Train_loss : 0.717184, TrainAcc : 0.675755 , Val loss : 0.682208 , Val acc : 0.593750, time : 1.56643s\n",
      "epoch: 100 , Train_loss : 0.706084, TrainAcc : 0.686465 , Val loss : 0.660894 , Val acc : 0.593750, time : 1.58840s\n",
      "epoch: 110 , Train_loss : 0.696028, TrainAcc : 0.690360 , Val loss : 0.641716 , Val acc : 0.601562, time : 1.71144s\n",
      "epoch: 120 , Train_loss : 0.686839, TrainAcc : 0.687439 , Val loss : 0.624417 , Val acc : 0.613281, time : 1.66106s\n",
      "epoch: 130 , Train_loss : 0.678381, TrainAcc : 0.693281 , Val loss : 0.608774 , Val acc : 0.621094, time : 1.60972s\n",
      "epoch: 140 , Train_loss : 0.670547, TrainAcc : 0.698150 , Val loss : 0.594595 , Val acc : 0.628906, time : 1.56782s\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "ep = 0.00001\n",
    "\n",
    "##--------------------Data Preprocesseing -------------------------------\n",
    "\n",
    "Train_X = []\n",
    "Train_Y = []\n",
    "y=0\n",
    "for a,b,file in os.walk(\"horse-or-human/train\"):\n",
    "    for i in file:\n",
    "        str1 = a+\"/\"+str(i)\n",
    "        t = cv2.imread(str1)\n",
    "        t = cv2.cvtColor(t,cv2.COLOR_BGR2GRAY)\n",
    "        t = t/255\n",
    "        t = t.reshape(10000)\n",
    "        Train_X.append(t)\n",
    "        Train_Y.append(y-1)\n",
    "    y+=1\n",
    "\n",
    "Test_X = []\n",
    "Test_Y = []\n",
    "y=0\n",
    "for a,b,file in os.walk(\"horse-or-human/validation\"):\n",
    "    for i in file:\n",
    "        str1 = a+\"/\"+str(i)\n",
    "        t = cv2.imread(str1)\n",
    "        t = cv2.cvtColor(t,cv2.COLOR_BGR2GRAY)\n",
    "        t = t/255\n",
    "        t = t.reshape(10000)  #vectorization\n",
    "        Test_X.append(t)\n",
    "        Test_Y.append(y-1)\n",
    "    y+=1\n",
    "print(\"loading the train img Complete : %d\"%(len(Train_X)))\n",
    "print(\"loading the validation img Complete : %d\"%(len(Test_X)))\n",
    "\n",
    "#------------------------ function definition------------------------------------\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1/(1+np.exp(-z))\n",
    "\n",
    "def gradient_decent_vectorization(w,b,rate):\n",
    "    w = np.asarray(w).reshape(len(Train_X[0]))\n",
    "    x = np.asarray(Train_X).T\n",
    "    oz = sigmoid(np.matmul(w,x)+b)  # x : 10000 X 1027 matrix,    oz : 1027 vector\n",
    "    nw = w - rate*(1/len(Train_X[0]))*np.matmul(x,oz-Train_Y)\n",
    "    nb = b - rate*(1/len(Train_X[0]))*np.sum(oz-Train_Y)\n",
    "    return (nw,nb)\n",
    "\n",
    "def cross_entropy(py,y):\n",
    "    sum = 0\n",
    "    for i in range(len(py)):\n",
    "        sum += -y[i]*np.log(py[i]+ep)-(1-y[i])*np.log(1-py[i]+ep)\n",
    "    return sum/len(py)\n",
    "\n",
    "def data_forward(prevX,w,b):  # 다음층 전달\n",
    "    w = np.asarray(w)\n",
    "    x = np.asarray(prevX).T\n",
    "    return sigmoid(np.matmul(w,x)+b)\n",
    "\n",
    "\n",
    "def back_propagation(W1,W2,W3,b1,b2,b3,X1,X2,X3,rate):\n",
    "    loss3 = (1/len(X3))*np.matmul(X2,X3-Train_Y)\n",
    "    nW3 = W3 - rate * loss3                            # 출력층 가중치 업데이트\n",
    "    nb3 = b3 - rate * (1/len(X3))*np.sum(X3-Train_Y)   # 출력층 바이어스 업데이트\n",
    "    X2 = np.asarray(X2)\n",
    "    X1 = np.asarray(X1)\n",
    "    print(\"loss3 :\")\n",
    "    print(np.asarray(loss3).shape)\n",
    "    loss2 = float(np.matmul(loss3,W3.T))*np.asarray(np.matmul(X2.T,1-X2))\n",
    "    loss2 = np.asarray(loss2)\n",
    "    print(\"loss2 :\")\n",
    "    print(loss2.shape)\n",
    "    loss1 = float(np.matmul(loss3, W3.T)) * np.asarray(np.matmul(X2.T, 1 - X2))\n",
    "    loss1 = np.asarray(loss2)\n",
    "    print(\"loss2 :\")\n",
    "    print(loss2.shape)\n",
    "    return (W1,W2,nW3,b1,b2,nb3)\n",
    "\n",
    "\n",
    "#----------------------- ready to learning ----------------------------------\n",
    "\n",
    "np.random.seed(3)\n",
    "w = np.random.normal(0.000001,0.000009,10000)\n",
    "\n",
    "b = 4\n",
    "rate = 0.021\n",
    "epoches = 500\n",
    "\n",
    "Train_Acc = []\n",
    "Train_Loss = []\n",
    "Val_Acc = []\n",
    "Val_Loss = []\n",
    "\n",
    "\n",
    "nodesize = 10\n",
    "w1 = []\n",
    "w2 = []\n",
    "w3 = np.random.normal(0.000001,0.000009,nodesize)  # 출력층 가중치\n",
    "\n",
    "for i in range(nodesize):\n",
    "    w1.append(np.random.normal(0.000001,0.000009,10000))\n",
    "    w2.append(np.random.normal(0.000001,0.000009,nodesize))\n",
    "\n",
    "b = 4\n",
    "b1 = 4\n",
    "b2 = 3\n",
    "b3 = 2\n",
    "\n",
    "rate = 0.021\n",
    "\n",
    "X1 = []   # 첫번째 층의 결과 X1 = sigmoid(TrainX*W1 + b1)\n",
    "X2 = []   # 두번째 층의 결과 X2 = sigmoid(X1*W2 + b2)\n",
    "X3 = []   # 출력 층의 결과 X3 = sigmoid(X2*W3 + b3)\n",
    "\n",
    "print(\"초기 input shape\")\n",
    "print(np.asarray(Train_X).T.shape)\n",
    "print(\"초기 weight1 shape\")\n",
    "print(np.asarray(w1).shape)\n",
    "print(\"초기 weight2 shape\")\n",
    "print(np.asarray(w2).shape)\n",
    "print(\"초기 weight3 shape\")\n",
    "print(np.asarray(w3).shape)\n",
    "# ---------------------------------------learning ----------------------------------------------------------------\n",
    "\n",
    "pastTime = time.time()\n",
    "for i in range(epoches):\n",
    "    x = np.asarray(Train_X).T\n",
    "    (nw,nb) = gradient_decent_vectorization(w,b,rate)\n",
    "    w = nw\n",
    "    b = nb\n",
    "\n",
    "    # input ==== > layer 1 (out : X1)\n",
    "    for m in range(nodesize):\n",
    "        X1=np.asarray(X1)\n",
    "    #    X1.append(data_forward(Train_X, w1[i], b1))\n",
    "    #print(np.asarray(X1).T.shape)\n",
    "\n",
    "    # layer 1 ==== > layer 2 (out : X2)\n",
    "    for n in range(nodesize):\n",
    "        X1 = np.asarray(X1)\n",
    "    #    X2.append(data_forward(np.asarray(X1).T, w2[i], b2))\n",
    "    #print(np.asarray(X2).T.shape)\n",
    "\n",
    "    # layer 2 ==== > Output Layer (out : X3)\n",
    "    X1 = np.asarray(X1)\n",
    "    #X3 = data_forward(np.asarray(X2).T, w3, b3)\n",
    "    #print(np.asarray(X3).T.shape)\n",
    "\n",
    "    #back_propagation(w1, w2, w3, b1, b2, b3, X1, X2, X3, rate)  # 역전파\n",
    "    #(nw1, nw2, nw3, nb1, nb2, nb3) = back_propagation(w1, w2, w3, b1, b2, b3, X1, X2, X3, rate)\n",
    "    #w1 = nw1\n",
    "    #w2 = nw2\n",
    "    #w3 = nw3\n",
    "    #b1 = nb1\n",
    "    #b2 = nb2\n",
    "    #b3 = nb3\n",
    "\n",
    "    #--- Train Cal------------------------------------\n",
    "    x = np.asarray(Train_X).T\n",
    "    predict_y = sigmoid(np.matmul(w,x)+b)\n",
    "    train_loss = cross_entropy(predict_y,Train_Y)\n",
    "    Train_Loss.append([i,train_loss])\n",
    "\n",
    "    for j in range(len(predict_y)):\n",
    "        if predict_y[j] > 0.5:\n",
    "            predict_y[j] = 1\n",
    "        else:\n",
    "            predict_y[j] = 0\n",
    "\n",
    "    acc_count=0\n",
    "    for k in range(len(predict_y)):\n",
    "        if predict_y[k] == Train_Y[k]:\n",
    "            acc_count+=1\n",
    "    train_acc = acc_count/len(predict_y)\n",
    "    Train_Acc.append([i,train_acc])\n",
    "\n",
    "    #--- Validation Cal ------------------------------\n",
    "    vx = np.asarray(Test_X).T\n",
    "    vpredict_y = sigmoid(np.matmul(w, vx) + b)\n",
    "    test_loss = cross_entropy(vpredict_y, Test_Y)\n",
    "    Val_Loss.append([i,test_loss])\n",
    "\n",
    "    for j in range(len(vpredict_y)):\n",
    "        if vpredict_y[j] > 0.5:\n",
    "            vpredict_y[j] = 1\n",
    "        else:\n",
    "           vpredict_y[j] = 0\n",
    "\n",
    "    acc_count=0\n",
    "    for l in range(len(vpredict_y)):\n",
    "        if vpredict_y[l] == Test_Y[l]:\n",
    "            acc_count+=1\n",
    "    test_acc = acc_count/len(vpredict_y)\n",
    "    Val_Acc.append([i,test_acc])\n",
    "\n",
    "    #---------------------------------------------------\n",
    "    if i%10 ==0 :\n",
    "        print(\"epoch: %d , Train_loss : %f, TrainAcc : %f , Val loss : %f , Val acc : %f, time : %0.5fs\" % (i,train_loss,train_acc,test_loss,test_acc,time.time()-pastTime))\n",
    "        pastTime = time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------result plot ---------------------------------\n",
    "\n",
    "listOfTrainLoss = np.asarray(Train_Loss)\n",
    "listOfTrainAcc = np.asarray(Train_Acc)\n",
    "listOfTestLoss = np.asarray(Val_Loss)\n",
    "listOfTestAcc = np.asarray(Val_Acc)\n",
    "plt.xlabel('epoch')\n",
    "plt.xlim(40,epoches)\n",
    "plt.plot(listOfTrainLoss[:,0],listOfTrainLoss[:,1],c=\"y\",label='TrainLoss')\n",
    "plt.plot(listOfTestLoss[:,0],listOfTestLoss[:,1],label='ValLoss')\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.show()\n",
    "plt.xlabel('epoch')\n",
    "plt.xlim(40,epoches)\n",
    "plt.plot(listOfTrainAcc[:,0],listOfTrainAcc[:,1],c=\"r\",label='TrainAcc')\n",
    "plt.plot(listOfTestAcc[:,0],listOfTestAcc[:,1],c=\"b\",label='ValAcc')\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.show()\n",
    "\n",
    "fig = plt.figure()\n",
    "col_labels = ['loss', 'acc']\n",
    "row_labels = ['training', 'validation']\n",
    "table_vals = [[round(Train_Loss[epoches-1][1],3),round(Train_Acc[epoches-1][1],3)],[round(Val_Loss[epoches-1][1],3),round(Val_Acc[epoches-1][1],3)]]\n",
    "\n",
    "the_table = plt.table(cellText=table_vals,colWidths=[0.1] * 2,rowLabels=row_labels,colLabels=col_labels)\n",
    "the_table.auto_set_font_size(False)\n",
    "the_table.set_fontsize(24)\n",
    "the_table.scale(4, 4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
